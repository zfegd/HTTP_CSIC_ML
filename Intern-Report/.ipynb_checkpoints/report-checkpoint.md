# Report on Internship

## Section 1 - Prior Experience in Computer Science/Data Science

    As a 2nd year Computing student from Imperial, I have encountered a decent amount of breadth in various aspects of Computing, but not much depth in particular areas of specialization. Having first discovered my passion for programming in Secondary 2, I had worked on small personal projects in Secondary School/JC, and knew that it was a subject I wanted to major in. However, computing has an assortment of different sub-fields, which I have not had the opportunity to explore in yet. Prior to starting my undergraduate studies, I had already contemplated 4 aspects of Computing where I might be particularly intersted in - Software Engineering, Artificial Intelligence, Data Science, and Cyber Security.

    This internship offered me the chance to explore one such area, that of Data Science. At university, we have not had any modules so far that would directly relate to the areas of Data Analytics that I would encounter at this internship. My closest experience I have had with working with Data Sets and Analyzing them would be at hackathons. While hackathons allow for exploration into many different areas which I would not normally do, foraying into data science without any prior experience was a daunting task, and the lack of proper guidance towards learning such skills was a little off-putting for me. Hence, going into the internship, I was a little hesitant as to how much I would enjoy doing data science, but was open to learning and finding out if it was something I could be truly passionate about working with in the future.

## Section 2 - What I read up on
    Without much experience or knowledge about data science in general, the first step was to delve into plenty of readings and research in order to have a better grasp of what to expect, and what I can do. After discussing and understanding the context from which I was coming into this internship, my supervisor, Shane, set aside a list of resources which he felt would be useful for me. The fact that python was being used was somewhat of a comfort, as I had worked with python in personal projects as well as school before. Whilst python is not my favorite language, I enjoy its simplicity and how powerful it can be while being straightforward to use. This would allow me to have to worry less about the coding, but more on the conceptual challenges which working with data might bring.
    
    As to ease into the internship, I first refreshed my knowledge, while learning new bits too, with Cornell's CS2043 course on Unix Tools and Scripting. Such skills are particularly useful day-to-day, as well as in processing data. Following that, I moved on to read up on Stanford's CS246 course on Mining Massive Data Sets. This course offered knowledge pertaining more towards actually working with data sets, and offered quite a lot of theoretical teachings of concepts. Certain aspects of it got a bit too mathematical for me to grasp, as I have not had much experience with linear algebra. However, the general concepts, such as PageRank, offered many fascinating insights which I have not considered before.
    
    Moving on from the academic lecture slides, I refreshed my coding skills by doing some simple challenges on leetcode. This was to refresh my knowledge of python, as I had not used it in a while. I also read up on pandas, which is used throughout the data science world. I have not used it before, so I followed a few tutorials in order to grasp the purpose behind using pandas. In order to gain some knowledge before embarking on my exploratory report, I also had to read up on different types of Machine Learning Classifiers and Evaluation Metrics. This also involved following some Scikit-learn tutorials, as it provides built-in libraries which would aid building classifiers and models to work with data.

## Section 3 - Beginning
    After reading through many tutorials online, it was important to actually code stuff out to practice the skills hands-on. I did so by following tutorials on how to implement different features of pandas, such as to manipulate data frames. Thereafter, I followed the tutorial on scikit-learn, regarding the 20 News Group Tutorial. This tutorial did not just introduce working with text data, but the entire pipeline which can be used in data analytics.

## Section 4 - NSL KDD Dataset
    This section involves diving into the data set, where I set about on an exploratory process before settling on a pathway to which my final report was developed. As I was not sure what to expect at the start, I started by working purely on the Small Training Set provided, rather than the full training set (which was about 100x as large). I explored both using binary classifiers and multi-class classifiers, also with respect to multiple features, or even all the features. As there was no "small test set", I used a random sampling of the entire test set, so as to ensure the test set was not bigger than the training set. Having initially not done any folding, the test set was not equally representative of the population as the training set. This caused possibly skewed data, where the binary classifier was performing a lot worse than the multi-class classifier. Despite that, the multi-class classifier was still performing at a rate of only 0.7, where I expected better results. This exploration allowed me to find a direction to which I took my report towards, which was using a multi-class classifier on the entire training set. As the training set was a lot bigger, I had to forgo using SVCs as they were simply not cost-efficient. By splitting the data into training and test sets to represent the proportion of the population equally, I managed to get much better results as well. Through a little more iteration, I managed to find a best model suitable to classify the accesses into attacks or normal accesses at a 99.7% success rate.

## Section 5 - HTTP CSIC Dataset
    Unlike the previous dataset, this dataset involved a little more pre-processing in order to "clean" the data up to be used. This refers to both creating tables to work with, and dropping columns which would not be useful. I would also have more opportunities to manually parse through the data, using feature selection and creation to try and improve the model that I would create. Domain knowledge would come in more useful in this scenario, in order to try and create better features. To gradually progress from the previous areas I worked with, I am trying to begin with creating a binary classifier, before adjusting to create an anomaly detector instead.

## Section 6 - Conclusion and Future
    Having only spent a short 5 weeks at IPA, I feel like I have learnt a lot about not just the culture of the corporation, but developed a lot of technical skills and insights which university has not offered to me so far. While I did not manage to really get my "hands dirty" with less academic data sets to reflect real world problems, I feel a lot more confident about tackling data sets and using them for analytical purposes. I'm really appreciative of the experience I've had here, and am able to make a much better informed opinion as to how I feel about data science in general. While I have not had the opportunity yet to explore Cyber Security and Artificial Intelligence, I think that Data Science is definitely an area I am open to developing my skills further in, and to potentially work in in the future.